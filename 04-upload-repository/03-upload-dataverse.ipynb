{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Upload to Dataverse\n",
    "\n",
    "\n",
    "Using the dataverse native and sword APIs. Log into https://dataverse.harvard.edu to create an API key.\n",
    "\n",
    "  - Create a draft dataset revision on the dataverse (edit > edit metadata > change something > save)\n",
    "  - Run this notebook\n",
    "  - Visit dataverse and publish the revised draft\n",
    "  \n",
    "The sword api is needed to delete files as this hasn't been implemented in the native api yet. The native api handles all the file uploading and metadata (that the sword only offers limited support for).\n",
    "\n",
    "  - sword api: http://guides.dataverse.org/en/4.8.6/api/sword.html#delete-a-file-by-database-id\n",
    "  - native api: http://guides.dataverse.org/en/4.8.6/api/native-api.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import config\n",
    "import json\n",
    "import keys\n",
    "import logging as lg\n",
    "import os\n",
    "import osmnx as ox\n",
    "import requests\n",
    "import time\n",
    "import xmltodict\n",
    "import zipfile\n",
    "\n",
    "ox.config(log_console=True, log_file=True, log_filename='upload-dataverse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "delete_existing_files = False #only make true on the first run to clear out everything from the draft\n",
    "debug_mode = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configure the dataverse upload\n",
    "server = 'https://dataverse.harvard.edu'\n",
    "attempts_max = 3     #how many times to re-try same file upload after error before giving up\n",
    "pause_error = 10     #seconds to pause after an error\n",
    "pause_normal = 2    #seconds to pause between uploads\n",
    "upload_timeout = 300 #how long to set the timeout for upload post requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load state fips code to state name dict\n",
    "with open('input_data/states_by_fips.json') as f:\n",
    "    fips_to_state = json.load(f)\n",
    "\n",
    "abbrev_state = {v['abbreviation']:v['name'] for k, v in fips_to_state.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define what to upload\n",
    "manifests = [{\n",
    "              'folder':'{}/counties-street_networks-graphml', #folder of zip files to upload\n",
    "              'file_desc':'Zip file contains the GraphML files of {}\\'s counties\\' street networks.',\n",
    "              'file_tags':['Data', 'GraphML', 'Counties', 'Street Network']\n",
    "             },\n",
    "             {\n",
    "              'folder':'{}/counties-street_networks-shapefiles',\n",
    "              'file_desc':'Zip file contains the shapefiles of {}\\'s counties\\' street networks.',\n",
    "              'file_tags':['Data', 'Shapefiles', 'Counties', 'Street Network'] \n",
    "             },\n",
    "             {\n",
    "              'folder':'{}/counties-street_networks-node_edge_lists',\n",
    "              'file_desc':'Zip file contains the node/edge lists of {}\\'s counties\\' street networks.',\n",
    "              'file_tags':['Data', 'Node/Edge Lists', 'Counties', 'Street Network'] \n",
    "             },\n",
    "             {\n",
    "              'folder':'{}/cities-street_networks-graphml',\n",
    "              'file_desc':'Zip file contains the GraphML files of {}\\'s cities/towns\\' street networks.',\n",
    "              'file_tags':['Data', 'GraphML', 'Cities/Towns', 'Street Network']\n",
    "             },\n",
    "             {\n",
    "              'folder':'{}/cities-street_networks-shapefiles',\n",
    "              'file_desc':'Zip file contains the shapefiles of {}\\'s cities/towns\\' street networks.',\n",
    "              'file_tags':['Data', 'Shapefiles', 'Cities/Towns', 'Street Network'] \n",
    "             },\n",
    "             {\n",
    "              'folder':'{}/cities-street_networks-node_edge_lists',\n",
    "              'file_desc':'Zip file contains the node/edge lists of {}\\'s cities/towns\\' street networks.',\n",
    "              'file_tags':['Data', 'Node/Edge Lists', 'Cities/Towns', 'Street Network'] \n",
    "             },\n",
    "             {\n",
    "              'folder':'{}/neighborhoods-street_networks-graphml',\n",
    "              'file_desc':'Zip file contains the GraphML files of {}\\'s neighborhoods\\' street networks.',\n",
    "              'file_tags':['Data', 'GraphML', 'Neighborhoods', 'Street Network']\n",
    "             },\n",
    "             {\n",
    "              'folder':'{}/neighborhoods-street_networks-shapefiles',\n",
    "              'file_desc':'Zip file contains the shapefiles of {}\\'s neighborhoods\\' street networks.',\n",
    "              'file_tags':['Data', 'Shapefiles', 'Neighborhoods', 'Street Network'] \n",
    "             },\n",
    "             {\n",
    "              'folder':'{}/neighborhoods-street_networks-node_edge_lists',\n",
    "              'file_desc':'Zip file contains the node/edge lists of {}\\'s neighborhoods\\' street networks.',\n",
    "              'file_tags':['Data', 'Node/Edge Lists', 'Neighborhoods', 'Street Network'] \n",
    "             },\n",
    "             {\n",
    "              'folder':'{}/urbanized_areas-street_networks-graphml',\n",
    "              'file_desc':'Zip file contains the GraphML files of {}\\'s urbanized areas\\' street networks.',\n",
    "              'file_tags':['Data', 'GraphML', 'Urbanized Areas', 'Street Network']\n",
    "             },\n",
    "             {\n",
    "              'folder':'{}/urbanized_areas-street_networks-shapefiles',\n",
    "              'file_desc':'Zip file contains the shapefiles of {}\\'s urbanized areas\\' street networks.',\n",
    "              'file_tags':['Data', 'Shapefiles', 'Urbanized Areas', 'Street Network'] \n",
    "             },\n",
    "             {\n",
    "              'folder':'{}/urbanized_areas-street_networks-node_edge_lists',\n",
    "              'file_desc':'Zip file contains the node/edge lists of {}\\'s urbanized areas\\' street networks.',\n",
    "              'file_tags':['Data', 'Node/Edge Lists', 'Urbanized Areas', 'Street Network'] \n",
    "             },\n",
    "             {\n",
    "              'folder':'{}/tracts-street_networks-graphml',\n",
    "              'file_desc':'Zip file contains the GraphML files of {}\\'s census tracts\\' street networks.',\n",
    "              'file_tags':['Data', 'GraphML', 'Census Tracts', 'Street Network']\n",
    "             },\n",
    "             {\n",
    "              'folder':'{}/tracts-street_networks-shapefiles',\n",
    "              'file_desc':'Zip file contains the shapefiles of {}\\'s census tracts\\' street networks.',\n",
    "              'file_tags':['Data', 'Shapefiles', 'Census Tracts', 'Street Network'] \n",
    "             },\n",
    "             {\n",
    "              'folder':'{}/tracts-street_networks-node_edge_lists',\n",
    "              'file_desc':'Zip file contains the node/edge lists of {}\\'s census tracts\\' street networks.',\n",
    "              'file_tags':['Data', 'Node/Edge Lists', 'Census Tracts', 'Street Network'] \n",
    "             }]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# zip a staged zipped file, open it, and return the buffer\n",
    "# this will double-zip the zip files because dataverse unzips zip files when they are uploaded\n",
    "# the result is that dataverse hosts the original zipped file\n",
    "def get_file_to_upload(file_path, archive_name, upload_file='temp_upload.zip'):\n",
    "    \n",
    "    upload_filepath = '{}/{}'.format(config.staging_folder, upload_file)\n",
    "    file_path = file_path.format(config.staging_folder)\n",
    "    \n",
    "    zf = zipfile.ZipFile(file=upload_filepath, mode='w')\n",
    "    zf.write(file_path, arcname=archive_name)\n",
    "    zf.close()\n",
    "    \n",
    "    file = {'file':open(upload_filepath, mode='rb')}\n",
    "    return file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configure the file description and tags that appear on dataverse\n",
    "def get_payload_to_upload(file_desc, file_tags, filename):\n",
    "    \n",
    "    # convert 2-digit state abbreviation to full state name and add it to description and tags\n",
    "    state_abbrev = filename[3:5]\n",
    "    state_name = abbrev_state[state_abbrev]\n",
    "    file_desc = file_desc.format(state_name)\n",
    "    file_tags = file_tags + [state_name]\n",
    "    \n",
    "    params = {'description':file_desc, 'categories':file_tags}\n",
    "    param_str = json.dumps(params)\n",
    "    payload = {'jsonData':param_str}\n",
    "    return payload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# upload a new file to a dataverse dataset\n",
    "def upload_new_file(folder, filename, doi, file_desc, file_tags, attempt_count=1):\n",
    "\n",
    "    file_path = '{}/{}'.format(folder, filename)\n",
    "    response = None\n",
    "    \n",
    "    # set up the api endpoint, open the file, and make the payload\n",
    "    endpoint = 'api/v1/datasets/:persistentId/add?persistentId={}&key={}'.format(doi, keys.api_key)\n",
    "    url = '{}/{}'.format(server, endpoint)\n",
    "    file = get_file_to_upload(file_path=file_path, archive_name=filename)\n",
    "    payload = get_payload_to_upload(file_desc=file_desc, file_tags=file_tags, filename=filename)\n",
    "    \n",
    "    try:\n",
    "        # upload the file to the server\n",
    "        ox.log('uploading \"{}\" to {}'.format(filename, doi))\n",
    "        \n",
    "        if debug_mode:\n",
    "            pass\n",
    "        else:\n",
    "            start_time = time.time()\n",
    "            session = requests.Session()\n",
    "            response = session.post(url, data=payload, files=file, timeout=upload_timeout)\n",
    "            ox.log('response {} in {:,.1f} seconds'.format(response.status_code, time.time()-start_time))\n",
    "\n",
    "            # check if the server response is ok, if not, throw exception\n",
    "            response_json = response.json()\n",
    "            if 'status' in response_json and not response_json['status'] == 'OK':\n",
    "                raise Exception(response_json['message'])\n",
    "\n",
    "            session.close()\n",
    "            time.sleep(pause_normal)\n",
    "    \n",
    "    except Exception as e:\n",
    "        \n",
    "        # if any exception is thrown, log it, and retry the upload if we haven't exceeded max number of tries\n",
    "        ox.log(e, level=lg.ERROR)\n",
    "        session.close()\n",
    "        time.sleep(pause_error)\n",
    "        \n",
    "        if attempt_count < attempts_max:\n",
    "            attempt_count += 1\n",
    "            ox.log('re-trying (attempt {} of {})'.format(attempt_count, attempts_max))\n",
    "            response = upload_new_file(folder, filename, doi, file_desc, file_tags, attempt_count=attempt_count)\n",
    "        else:\n",
    "            ox.log('no more attempts for this file, we give up', level=lg.WARN)\n",
    "    \n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all the filenames that currently exist in the DRAFT dataset\n",
    "def get_uploaded_draft_filenames(dataset_doi):\n",
    "    \n",
    "    endpoint = 'api/v1/datasets/:persistentId/versions/:draft/files?key={}&persistentId={}'\n",
    "    url = '{}/{}'.format(server, endpoint).format(keys.api_key, dataset_doi)\n",
    "    response = requests.get(url)\n",
    "    response_json = response.json()\n",
    "    \n",
    "    if 'data' in response_json and len(response_json['data']) > 0:\n",
    "        uploaded_files = response_json['data']\n",
    "        uploaded_filenames = [file['dataFile']['filename'] for file in uploaded_files]\n",
    "    else:\n",
    "        uploaded_filenames = []\n",
    "    \n",
    "    return uploaded_filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all the filenames that currently exist in the latest published dataset\n",
    "def get_published_files(dataset_doi):\n",
    "    \n",
    "    endpoint = 'api/v1/datasets/:persistentId/versions/:latest-published/files?key={}&persistentId={}'\n",
    "    url = '{}/{}'.format(server, endpoint).format(keys.api_key, dataset_doi)\n",
    "    response = requests.get(url)\n",
    "    response_json = response.json()\n",
    "    \n",
    "    if 'data' in response_json and len(response_json['data']) > 0:\n",
    "        filelist = response_json['data']\n",
    "        published_files = {file['dataFile']['filename']:file['dataFile']['id'] for file in filelist}\n",
    "    else:\n",
    "        published_files = {}\n",
    "    \n",
    "    return published_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_dataset_files(doi=keys.my_doi):\n",
    "    \"\"\"\n",
    "    Delete all files from draft dataset at the given DOI.\n",
    "    \"\"\"\n",
    "\n",
    "    host = 'dataverse.harvard.edu'\n",
    "    url_statement = 'https://{}/dvn/api/data-deposit/v1.1/swordv2/statement/study/{}'.format(host, doi)\n",
    "    auth = (keys.api_key, None)\n",
    "    response = requests.get(url_statement, auth=auth)\n",
    "    assert response.status_code == 200\n",
    "    \n",
    "    response_dict = xmltodict.parse(response.text)\n",
    "    files = response_dict['feed']['entry']\n",
    "    ox.log('There are {} files to delete'.format(len(files)))\n",
    "    all_start_time = time.time()\n",
    "    \n",
    "    i = 0\n",
    "    for file in files:\n",
    "        \n",
    "        file_name = file['id'].split('/')[-1]\n",
    "        file_id = file['id'].split('/')[-2]\n",
    "        url_delete = 'https://{}/dvn/api/data-deposit/v1.1/swordv2/edit-media/file/{}'.format(host, file_id)\n",
    "        auth = (keys.api_key, None)\n",
    "        \n",
    "        start_time = time.time()\n",
    "        response = requests.delete(url_delete, auth=auth)\n",
    "        ox.log('{} Deleted \"{}\" in {:.1f} seconds'.format(response, file_name, time.time()-start_time))\n",
    "        assert response.status_code == 204\n",
    "        i += 1\n",
    "    \n",
    "    ox.log('Deleted {} files in {} seconds'.format(i, int(time.time()-all_start_time)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "ox.log('script started')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "290\n",
      "765\n"
     ]
    }
   ],
   "source": [
    "# what files have already been uploaded to the draft?\n",
    "already_uploaded = get_uploaded_draft_filenames(keys.my_doi)\n",
    "\n",
    "# what files exist in the published version of the dataset?\n",
    "published_files = get_published_files(keys.my_doi)\n",
    "\n",
    "print(len(published_files))\n",
    "print(len(already_uploaded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "if delete_existing_files:\n",
    "    \n",
    "    # delete all the existing (carried-over) files in the draft dataset\n",
    "    delete_dataset_files()\n",
    "    \n",
    "    # what files have already been uploaded to the draft?\n",
    "    already_uploaded = get_uploaded_draft_filenames(keys.my_doi)\n",
    "\n",
    "    # what files exist in the published version of the dataset?\n",
    "    published_files = get_published_files(keys.my_doi)\n",
    "\n",
    "    print(len(published_files))\n",
    "    print(len(already_uploaded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "for manifest in manifests:\n",
    "    \n",
    "    ox.log('manifest={}'.format(manifest))\n",
    "    \n",
    "    # get list of filenames in this folder to upload\n",
    "    folder = manifest['folder']\n",
    "    file_desc = manifest['file_desc']\n",
    "    file_tags = manifest['file_tags']\n",
    "\n",
    "    # upload each file\n",
    "    filenames = os.listdir(folder.format(config.staging_folder))\n",
    "    for filename in filenames:\n",
    "        \n",
    "        # if it's not already in the draft revision on the server, upload it\n",
    "        if not filename in already_uploaded:\n",
    "            response = upload_new_file(folder, filename, keys.my_doi, file_desc, file_tags)\n",
    "        else:\n",
    "            ox.log('skipping \"{}\" because it is already on the server'.format(filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "ox.log('script finished in {} seconds'.format(int(time.time()-start_time)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
